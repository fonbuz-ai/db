import streamlit as st
import requests
import os
import re
from urllib.parse import unquote
import zipfile
import io
from bs4 import BeautifulSoup

# === åŒ¯å…¥æœå°‹å‡½å¼åº« ===
from ddgs import DDGS
import arxiv
from googlesearch import search as google_unofficial_search
from googleapiclient.discovery import build # Google Official
from serpapi import GoogleSearch # SerpApi

# === è¨­å®šé é¢ ===
st.set_page_config(page_title="å…¨èƒ½ PDF æœå°‹ç¥å™¨ (è‡ªå‹•å…¨é¸ç‰ˆ)", page_icon="ğŸ•µï¸", layout="wide")

# === æ ¸å¿ƒï¼šé€šç”¨å·¥å…·èˆ‡ä¸‹è¼‰åŠŸèƒ½ ===
def get_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
    }

def get_filename_from_cd(cd):
    if not cd: return None
    fname = re.findall('filename=(.+)', cd)
    return fname[0].replace('"', '') if fname else None

def download_file(url, folder="downloads", progress_bar=None):
    """ä¸‹è¼‰å–®ä¸€æª”æ¡ˆï¼Œè‹¥æä¾› progress_bar å‰‡é¡¯ç¤ºé€²åº¦"""
    if not os.path.exists(folder): os.makedirs(folder)
    try:
        response = requests.get(url, stream=True, headers=get_headers(), timeout=20)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        filename = get_filename_from_cd(response.headers.get('content-disposition'))
        if not filename: filename = unquote(url.split("/")[-1])
        filename = re.sub(r'[\\/*?:"<>|]', "", filename)
        if len(filename) > 50: filename = filename[:50]
        if not filename.lower().endswith('.pdf'): filename += ".pdf"
        
        file_path = os.path.join(folder, filename)
        
        downloaded = 0
        with open(file_path, 'wb') as f:
            for chunk in response.iter_content(8192):
                f.write(chunk)
                downloaded += len(chunk)
                if progress_bar and total_size > 0:
                    progress_bar.progress(min(downloaded/total_size, 1.0), text=f"ä¸‹è¼‰ä¸­: {filename} ({int(downloaded/total_size*100)}%)")
        return True, file_path, None
    except Exception as e:
        return False, None, str(e)

# === æ ¸å¿ƒï¼šç‰¹æ®Šç¶²ç«™çˆ¬èŸ²ç­–ç•¥ ===
def search_yabook(query, max_results):
    """åˆ©ç”¨æœå°‹å¼•æ“çš„ site: èªæ³•ä¾†æŠ“å–é›…æ›¸ï¼Œçµæœæ›´æº–ç¢ºä¸”å®Œå…¨ä¸å¡é “"""
    results = []
    try:
        # æˆ‘å€‘ç›´æ¥åˆ©ç”¨å…é‡‘é‘°çš„ DDGS (DuckDuckGo) ä¾†åŸ·è¡Œ site: æœå°‹ï¼Œé€™èˆ‡ Google site: æ•ˆæœæ¥µåº¦ç›¸ä¼¼
        search_query = f"site:yabook.org {query}"
        
        for r in DDGS().text(search_query, max_results=max_results):
            # ç¨å¾®æ¸…ç†ä¸€ä¸‹æ¨™é¡Œï¼ŒæŠŠæœå°‹å¼•æ“å¸¶å…¥çš„ç¶²ç«™å¾Œç¶´æ‹¿æ‰ï¼Œè®“ç•«é¢æ›´ä¹¾æ·¨
            title = r.get('title', '')
            title = title.replace(' - é›…ä¹¦', '').replace(' | é›…ä¹¦', '').replace(' - é›…æ›¸', '')
            
            results.append({
                "title": title[:60] + "..." if len(title) > 60 else title, 
                "link": r.get('href'), 
                "source": "é›…æ›¸ (Yabook)", 
                "type": "webpage" # ä¾ç„¶æ¨™è¨˜ç‚ºç¶²é ï¼Œåœ¨å³ä¸‹å€å¡Šé¡¯ç¤ºè¶…é€£çµ
            })
    except Exception as e: 
        st.error(f"é›…æ›¸ (Yabook) æœå°‹éŒ¯èª¤: {e}")
    return results

def search_oceanofpdf(query, max_results):
    results = []
    base_url = "https://oceanofpdf.com/"
    search_url = f"{base_url}?s={query.replace(' ', '+')}"
    try:
        response = requests.get(search_url, headers=get_headers(), timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            for i, article in enumerate(soup.find_all('article')):
                if i >= max_results: break
                title_tag = article.find('h2', class_='title')
                if title_tag and title_tag.find('a'):
                    results.append({"title": title_tag.get_text(strip=True), "link": title_tag.find('a')['href'], "source": "OceanofPDF", "type": "webpage"})
    except Exception as e: st.error(f"OceanofPDF éŒ¯èª¤: {e}")
    return results

def search_annas_archive(query, max_results):
    results = []
    base_url = "https://annas-archive.li"
    search_url = f"{base_url}/search?q={query.replace(' ', '+')}"
    try:
        response = requests.get(search_url, headers=get_headers(), timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            count = 0
            for link in soup.find_all('a', href=True):
                if '/md5/' in link['href']:
                    title = link.get_text(strip=True)
                    if len(title) > 5:
                        full_link = base_url + link['href'] if link['href'].startswith('/') else link['href']
                        results.append({"title": title[:60] + "...", "link": full_link, "source": "Anna's Archive", "type": "webpage"})
                        count += 1
                        if count >= max_results: break
    except Exception as e: st.error(f"Anna's Archive éŒ¯èª¤: {e}")
    return results

# === æ ¸å¿ƒï¼šAPI æœå°‹ç­–ç•¥ ===
def search_duckduckgo(query, max_results):
    results = []
    try:
        for r in DDGS().text(f"{query} filetype:pdf", max_results=max_results):
            results.append({"title": r.get('title'), "link": r.get('href'), "source": "DuckDuckGo", "type": "pdf"})
    except Exception as e: st.error(f"DuckDuckGo éŒ¯èª¤: {e}")
    return results

def search_arxiv_lib(query, max_results):
    results = []
    try:
        search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)
        for r in arxiv.Client().results(search):
            results.append({"title": f"[è«–æ–‡] {r.title}", "link": r.pdf_url, "source": "arXiv", "type": "pdf"})
    except Exception as e: st.error(f"arXiv éŒ¯èª¤: {e}")
    return results

def search_google_unofficial(query, max_results):
    results = []
    try:
        for r in google_unofficial_search(f"{query} filetype:pdf", num=max_results, advanced=True):
            results.append({"title": r.title, "link": r.url, "source": "Google (Unofficial)", "type": "pdf"})
    except Exception as e: st.error(f"Google (éå®˜æ–¹) éŒ¯èª¤: {e}")
    return results

def search_google_official(query, api_key, cse_id, max_results):
    results = []
    try:
        res = build("customsearch", "v1", developerKey=api_key).cse().list(q=query, cx=cse_id, fileType='pdf', num=max_results).execute()
        for item in res.get('items', []):
            results.append({"title": item['title'], "link": item['link'], "source": "Google API", "type": "pdf"})
    except Exception as e: st.error(f"Google API éŒ¯èª¤: {e}")
    return results

def search_serpapi(query, api_key, max_results):
    results = []
    try:
        data = GoogleSearch({"engine": "google", "q": f"{query} filetype:pdf", "api_key": api_key, "num": max_results}).get_dict()
        for item in data.get("organic_results", []):
            results.append({"title": item.get('title'), "link": item.get('link'), "source": "SerpApi", "type": "pdf"})
    except Exception as e: st.error(f"SerpApi éŒ¯èª¤: {e}")
    return results

# === ä¸»ä»‹é¢é‚è¼¯ ===
def main():
    # --- å·¦å´é‚Šæ¬„ï¼šæœå°‹è¨­å®š ---
    st.sidebar.title("âš™ï¸ æœå°‹è¨­å®š")
    st.sidebar.markdown("**é¸æ“‡æœå°‹å¼•æ“ (é è¨­å…¨é¸)**")
    
    engine_options = [
        "DuckDuckGo (æ¨è–¦/å…é‡‘é‘°)", 
        "arXiv (å­¸è¡“è«–æ–‡/å…é‡‘é‘°)", 
        "OceanofPDF (ç¶²é /å…é‡‘é‘°)",
        "Anna's Archive (ç¶²é /å…é‡‘é‘°)",
        "é›…æ›¸ Yabook (ç¶²é /å…é‡‘é‘°)",
        "Google (éå®˜æ–¹/æ˜“è¢«æ“‹)", 
        "Google Official API (éœ€é‡‘é‘°)", 
        "SerpApi (éœ€é‡‘é‘°)"
    ]
    
    selected_engines = []
    # ä½¿ç”¨è¿´åœˆç”¢ç”Ÿ Checkboxï¼Œä¸¦ä¸” value=True è®“å®ƒå…§å®šæ‰“å‹¾
    for engine in engine_options:
        if st.sidebar.checkbox(engine, value=True):
            selected_engines.append(engine)
    
    api_key, cse_id, serp_key = "", "", ""
    engine_str = "".join(selected_engines) 
    
    if "Google Official API" in engine_str:
        st.sidebar.warning("å·²å•Ÿç”¨ Google APIï¼Œè«‹è¼¸å…¥é‡‘é‘°ï¼š")
        api_key = st.sidebar.text_input("Google API Key", type="password")
        cse_id = st.sidebar.text_input("Search Engine ID (CSE ID)", type="password")
        
    if "SerpApi" in engine_str:
        st.sidebar.warning("å·²å•Ÿç”¨ SerpApiï¼Œè«‹è¼¸å…¥é‡‘é‘°ï¼š")
        serp_key = st.sidebar.text_input("SerpApi Key", type="password")

    # --- å³å´ä¸»ç•«é¢ ---
    st.title("ğŸ•µï¸ å…¨èƒ½ PDF æœå°‹ç¥å™¨ (è‡ªå‹•å…¨é¸æ‰¹æ¬¡ç‰ˆ)")
    st.markdown("---")

    col1, col2 = st.columns([4, 1])
    with col1:
        query = st.text_input("è¼¸å…¥é—œéµå­— (æ›¸å/è«–æ–‡å)", placeholder="ä¾‹å¦‚: åŸå­ç¿’æ…£")
    with col2:
        st.write(""); st.write("")
        start_search = st.button("ğŸ” é–‹å§‹æœå°‹", type="primary", use_container_width=True)

    if start_search and query:
        st.session_state.results = [] 
        max_res = 5
        
        for engine in selected_engines:
            with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {engine.split(' ')[0]} æœå°‹..."):
                if "DuckDuckGo" in engine:
                    st.session_state.results.extend(search_duckduckgo(query, max_res))
                elif "arXiv" in engine:
                    st.session_state.results.extend(search_arxiv_lib(query, max_res))
                elif "OceanofPDF" in engine:
                    st.session_state.results.extend(search_oceanofpdf(query, max_res))
                elif "Anna's Archive" in engine:
                    st.session_state.results.extend(search_annas_archive(query, max_res))
                elif "é›…æ›¸ Yabook" in engine:
                    st.session_state.results.extend(search_yabook(query, max_res))
                elif "Google (éå®˜æ–¹)" in engine:
                    st.session_state.results.extend(search_google_unofficial(query, max_res))
                elif "Google Official API" in engine and api_key and cse_id:
                    st.session_state.results.extend(search_google_official(query, api_key, cse_id, max_res))
                elif "SerpApi" in engine and serp_key:
                    st.session_state.results.extend(search_serpapi(query, serp_key, max_res))

    # === æœå°‹çµæœé¡¯ç¤º ===
    if 'results' in st.session_state and st.session_state.results:
        st.success(f"ğŸ‰ ç¸½å…±æ‰¾åˆ° {len(st.session_state.results)} å€‹ç›¸é—œçµæœï¼")
        st.markdown("---")
        
        # å°‡çµæœåˆ†ç‚º PDF å’Œ Webpage å…©é¡
        pdf_items = [item for item in st.session_state.results if item['type'] == 'pdf']
        web_items = [item for item in st.session_state.results if item['type'] == 'webpage']
        
        col_pdf, col_web = st.columns([1, 1])
        
        # ==========================================
        # å·¦æ¬„ / ä¸ŠåŠéƒ¨ï¼šç›´é€£ PDF å‹¾é¸ä¸‹è¼‰å€
        # ==========================================
        if pdf_items:
            st.subheader("ğŸ“„ ç›´é€£ PDF æª”æ¡ˆ (å¯æ‰¹æ¬¡æ‰“åŒ…)")
            st.write("å·²ç‚ºæ‚¨**é è¨­å…¨é¸**ï¼Œè«‹å–æ¶ˆä¸æƒ³ä¸‹è¼‰çš„é …ç›®ï¼š")
            
            selected_pdfs_to_download = []
            
            # ä½¿ç”¨ç¨ç«‹çš„ Checkbox åˆ—å‡º PDF
            for i, item in enumerate(pdf_items):
                is_checked = st.checkbox(
                    f"[{item['source']}] {item['title']}", 
                    value=True, # å…§å®šå‹¾é¸
                    key=f"pdf_chk_{i}"
                )
                if is_checked:
                    selected_pdfs_to_download.append(item)
            
            # ä¸‹è¼‰èˆ‡æ‰“åŒ…é‚è¼¯
            if selected_pdfs_to_download:
                st.write("")
                if st.button("â¬‡ï¸ é–‹å§‹ä¸‹è¼‰ä¸¦æ‰“åŒ…ç‚º ZIP å£“ç¸®æª”", type="primary"):
                    progress_bar = st.progress(0, text="æº–å‚™ä¸‹è¼‰...")
                    zip_buffer = io.BytesIO()
                    success_count = 0
                    
                    with zipfile.ZipFile(zip_buffer, "w") as zip_file:
                        for idx, item in enumerate(selected_pdfs_to_download):
                            progress_bar.progress(idx / len(selected_pdfs_to_download), text=f"è™•ç†ä¸­ ({idx+1}/{len(selected_pdfs_to_download)}): {item['title'][:20]}...")
                            success, path, error = download_file(item['link'], progress_bar=progress_bar)
                            
                            if success:
                                zip_file.write(path, os.path.basename(path))
                                success_count += 1
                            else:
                                st.error(f"âŒ ä¸‹è¼‰å¤±æ•—: {item['title']} (åŸå› : {error})")
                                
                    progress_bar.progress(1.0, text="è™•ç†å®Œæˆï¼")
                    
                    if success_count > 0:
                        st.success(f"âœ… æˆåŠŸæ‰“åŒ… {success_count} å€‹æª”æ¡ˆï¼è«‹é»æ“Šä¸‹æ–¹æŒ‰éˆ•å„²å­˜ã€‚")
                        st.download_button(
                            label="ğŸ’¾ å„²å­˜ ZIP æª”æ¡ˆè‡³é›»è…¦",
                            data=zip_buffer.getvalue(),
                            file_name="PDF_Downloads.zip",
                            mime="application/zip"
                        )
                    else:
                        st.error("æ‰€æœ‰å‹¾é¸çš„æª”æ¡ˆå‡ä¸‹è¼‰å¤±æ•—ã€‚")

        # ==========================================
        # å€åˆ†ç·šæˆ–å³æ¬„ï¼šç¶²é é€£çµé¡¯ç¤ºå€
        # ==========================================
        if web_items:
            st.markdown("---")
            st.subheader("ğŸŒ ç›¸é—œç¶²é è³‡æº (éœ€æ‰‹å‹•å‰å¾€)")
            st.warning("âš ï¸ **ä»¥ä¸‹é …ç›®éœ€é»æ“Šé€£çµå‰å¾€è©²ç¶²ç«™é€²è¡Œæ‰‹å‹•ä¸‹è¼‰æˆ–ç€è¦½ï¼š**")
            
            # ç›´æ¥æ¢åˆ—è³‡è¨Šèˆ‡è¶…é€£çµï¼Œä¸ä½¿ç”¨ Checkbox
            for item in web_items:
                with st.container(border=True):
                    st.markdown(f"**ä¾†æºï¼š** `{item['source']}`")
                    st.markdown(f"**æ¨™é¡Œï¼š** {item['title']}")
                    st.markdown(f"ğŸ”— **[é»æˆ‘å‰å¾€ä¸‹è¼‰é é¢]({item['link']})**")

if __name__ == "__main__":
    main()
